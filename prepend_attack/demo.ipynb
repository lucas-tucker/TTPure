{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e89edb68-ed02-4c2a-884e-254e8291ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface ASR dataset to be tested\n",
    "DATASET = 'google/fleurs'\n",
    "LANGUAGE = 'en_us'\n",
    "SPLIT = 'test'\n",
    "\n",
    "# Whisper model name, can be one of the following: tiny/tiny.en/base/base.en/small/small.en/medium/medium.en\n",
    "WHISPER_MODEL = 'tiny'\n",
    "\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e923c71-af64-4226-a69d-b188cf0efdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained OpenAI Whisper model\n",
    "import whisper\n",
    "\n",
    "model = whisper.load_model(WHISPER_MODEL).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51258184-1972-4182-84cd-5586c56dcf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
       "    num_rows: 647\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically download and load Huggingface dataset\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(DATASET, LANGUAGE, split=SPLIT, trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba8720ca-3afe-4d44-8e58-eaf872bd1dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/codespace/.cache/huggingface/datasets/downloads/extracted/1c070e5ac60ac03d2cd6d0c5709e46ab9f710b27de331557340ed64c0a44e94b/test/4780196329895653733.wav'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a random sample from the testset and print the reference \n",
    "import random, os\n",
    "idx = random.randint(0, len(dataset)-1)\n",
    "\n",
    "# Prepare the path of the selected utterance\n",
    "audio_path = os.path.join(os.path.dirname(dataset[idx]['path']), dataset[idx]['audio']['path'])\n",
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e218fee7-98e5-47c0-958b-8fe8ff677374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR reference: \"ironing damp clothes can help them dry many hotels have an iron and ironing board available for loan even if one is not present in the room\"\n"
     ]
    }
   ],
   "source": [
    "# The reference transcription for the utterance\n",
    "print(f'ASR reference: \"{dataset[idx][\"transcription\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b6c91f2-2450-4094-be1f-2a47ba100479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the utterance\n",
    "from whisper.audio import load_audio\n",
    "\n",
    "audio = load_audio(audio_path)\n",
    "audio = torch.from_numpy(audio).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2225abce-c2dc-4b0a-82b5-dade499fe9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv_gector/lib/python3.9/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Whisper transcription: \" Ironing damp clothes can help them dry. Many hotels have an iron and ironing board available for loan, even if one is not present in the room.\"\n"
     ]
    }
   ],
   "source": [
    "# Whisepr transcription for the original speech signal\n",
    "print(f'Default Whisper transcription: \"{model.transcribe(audio)[\"text\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22cfee35-292a-47d9-b273-73d187cf0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant universal acoustic adversarial attack segment (0.64 seconds in length)\n",
    "import numpy as np\n",
    "\n",
    "loaded_array = np.load(f'audio_attack_segments/{WHISPER_MODEL}.np.npy')\n",
    "audio_attack_segment = torch.from_numpy(loaded_array).to(audio.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3a26504-a501-4a67-9e10-6663b9d1c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend the learned universal attack segment to the original speech signal\n",
    "audio_with_prompts = torch.cat((audio_attack_segment, audio), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b88e8ac-83fe-4aab-b374-cee7968eb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper transcription with the learned attack segment: \"\"\n"
     ]
    }
   ],
   "source": [
    "# Whisepr transcription for the concatenated speech signal\n",
    "# Whisper is \"muted\" in this case\n",
    "print(f'Whisper transcription with the learned attack segment: \"{model.transcribe(audio_with_prompts)[\"text\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ff202fe-d91c-473d-ac0c-6e4d1b4b2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping audio to wav files\n",
    "\n",
    "import shutil\n",
    "from scipy.io import wavfile\n",
    "\n",
    "shutil.copyfile(audio_path, \"original_audio.wav\")\n",
    "audio_attack_array = audio_with_prompts.cpu().numpy()\n",
    "audio_attack_array = (audio_attack_array * 32767).astype(np.int16)\n",
    "wavfile.write('audio_with_attack.wav', 16000, audio_attack_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
